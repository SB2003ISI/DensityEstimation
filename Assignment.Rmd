---
title: ""
author: ""
output:
  pdf_document:
    number_sections: true
    citation_package: natbib
    latex_engine: pdflatex
    pandoc_args:
      - "--variable=colorlinks=true"
      - "--variable=urlcolor=blue"
      - "--variable=citecolor=blue"
    fig_width: 6.5
    fig_height: 4
  word_document: default
  html_document:
    df_print: paged
bibliography: references.assignment.bib
biblio-style: plainnat
link-citations: true
nocite: "@*"
header-includes:
  - \usepackage{mathtools}
  - \usepackage{amsmath}
  - \usepackage{amsthm}
  - \newtheorem{theorem}{Theorem}[section]
  - \newtheorem{corollary}[theorem]{Corollary}
  - \newtheorem{lemma}{Lemma}[section]
  - \newcommand{\lemref}[1]{Lemma~\ref{#1}}
  - \usepackage{hyperref}
  - \usepackage{marvosym}   
  - \usepackage{geometry}
  - \usepackage{float}
  - \usepackage{placeins}
  - \usepackage{booktabs}
  - \usepackage{colortbl}
  - \usepackage{caption}
  - \usepackage{fontawesome5}
  - \usepackage{academicons}
  - \usepackage{xcolor}
  - \usepackage{mdframed}
  - \definecolor{mygrey}{RGB}{240,240,240}
  - \geometry{paperwidth=6.5in, paperheight=9in, left=0.5in, right=0.5in, top=0.5in, bottom=0.5in}
  - \usepackage{setspace}
  - \onehalfspacing
  - \usepackage[fontsize=10pt]{scrextend}
  - \numberwithin{equation}{section}
  - \renewcommand{\sectionautorefname}{Section}
documentclass: extarticle
---

```{=latex}
\begin{center}
{\Large\textbf{Beyond the Basics: Advanced Density Approximation with Kernel, Edgeworth, and Saddle Point Approaches}}\\[1em]

\normalsize
Sourav Biswas$^*$\\
\scriptsize
\textit{Indian Statistical Institute, Kolkata}\\[1em]

\footnotesize
Roll: MD2320\\
$^*$ \Letter{} E-mail: \href{mailto:sourav3biswas2003@gmail.com}{sourav3biswas2003@gmail.com}
\end{center}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,comment=NA,fig.pos="H")
```

\begin{mdframed}[backgroundcolor=mygrey, linewidth=0pt, skipabove=10pt, skipbelow=10pt]
\raggedright
\textbf{Abstract} \\
Density estimation is a fundamental challenge in statistics, pivotal for uncovering the underlying structure of data. Several methodologies have emerged to address this problem, including nonparametric techniques such as kernel density estimation and asymptotic approaches like Edgeworth expansions and saddle point approximations. By reformulating the density of a random variable—derived from order statistics of an exponential distribution—as a sum of specific components, these methods can be rigorously applied. This work examines the theoretical foundations and practical implementation strategies underlying each approach, offering a unified perspective on their application to density estimation. The analysis provides a comprehensive framework for understanding the strengths and limitations inherent in these techniques, thereby contributing to the broader discourse on effective density estimation methods.

\vspace{5pt}
\par
\textbf{Keywords:} Cumulant Generating Function, Edgeworth Expansions, Hermite Polynomials, Kernel Density Estimation, Lagrange Polynomials, Saddle Point Approximation. 
\end{mdframed}

# Introduction {#sec:intro}

Density estimation is a fundamental problem in statistical inference, playing a pivotal role in understanding the underlying distribution of data. This report presents a comparative study of three distinct methodologies for density estimation: kernel density estimation, Edgeworth expansions, and saddle point approximations. Each method offers unique advantages and limitations, particularly in the context of estimating the density of a random variable defined as a function of sample ranges derived from order statistics.

Kernel density estimation (KDE) provides a nonparametric approach to density estimation by smoothing the empirical distribution through the use of a kernel function and a bandwidth parameter. This method is widely appreciated for its simplicity and intuitive appeal, yet its performance is critically dependent on the choice of the smoothing parameter.

Edgeworth expansions, on the other hand, leverage the cumulant generating function to develop asymptotic approximations of the distribution. This approach refines the normal approximation by incorporating skewness, kurtosis, and higher-order cumulants, thereby offering enhanced accuracy under certain conditions, especially when sample sizes are moderate to large.

Saddle point approximations utilize techniques from large deviations theory and Laplace’s method to achieve highly accurate density estimates even in scenarios involving small sample sizes. This method is particularly valuable for its precision and reliability in the tails of the distribution.

In the ensuing sections, we will formulate the density estimation problem in the context of order statistics drawn from an exponential distribution, and systematically apply each of the aforementioned methods to this framework. A detailed comparative analysis will then be conducted, focusing on theoretical properties as well as practical performance.

In \autoref{sec:setup}, we introduced the experimental framework upon which the subsequent analysis is based. \autoref{sec:kde} presents the kernel density estimation of the probability density function. \autoref{sec:edge} elaborates on the Edgeworth expansions method. Similarly, \autoref{sec:saddle} describes Saddle Point approximations. This study is designed to elucidate the specific conditions under which each method performs optimally and to offer recommendations for selecting the most appropriate technique for diverse statistical applications.

# Setup {#sec:setup}

Suppose $X_1,X_2,\ldots,X_{n+1}\overset{iid}{\sim}\mathrm{Exponential}\left(\mathrm{rate}=1\right)$. Each $X_i$ has the PDF and CDF given by,$$f_X\left(x\right)=e^{-x}\boldsymbol{1}\left(x>0\right)\,\,\mathrm{and}\,\,F_X\left(x\right)=\left(1-e^{-x}\right)\boldsymbol{1}\left(x>0\right).$$Then, joint density of $1$st and $\left(n+1\right)$th ordered statistics $f_{\left(X_{\left(1\right)},X_{\left(n+1\right)}\right)}\left(\centerdot,\centerdot\right)$ is given by,$$
\begin{aligned}
f_{\left(X_{\left(1\right)},X_{\left(n+1\right)}\right)}\left(x,y\right)
&=\frac{\left(n+1\right)!}{\left(n-1\right)!}f_X\left(x\right)\left(F_X\left(y\right)-F_X\left(x\right)\right)^{n-1}f_Y\left(y\right)\boldsymbol{1}\left(0<x<y<\infty\right)\\[1mm]
&=n\left(n+1\right)e^{-x}\left(1-e^{-y}-1+e^{-x}\right)^{n-1}e^{-y}\boldsymbol{1}\left(0<x<y<\infty\right)\\[1mm]
&=n\left(n+1\right)e^{-\left(x+y\right)}\left(e^{-x}-e^{-y}\right)^{n-1}\boldsymbol{1}\left(0<x<y<\infty\right).
\end{aligned}
$$ Say, $U=X_{\left(n+1\right)}-X_{\left(1\right)},V=X_{(1)}$. Which immediately implies, $X_{(n+1)}=U+V,X_{(1)}=V$. By making the transformation from $\left(X_{(1)},X_{(n+1)}\right)$ to $\left(U,V\right)$, we get the Jacobian of the transformation as,$$
\frac{\partial(x,y)}{\partial(u,v)}=\left(\begin{matrix}0 & 1\\1 & 1\end{matrix}\right).
$$Also, $0<X_{(1)}<X_{(n+1)}$ a.s. $\iff U>0,V>0$ a.s. Hence, joint pdf of $\left(U,V\right),f_{\left(U,V\right)}\left(\centerdot,\centerdot\right)$ is,$$
\begin{aligned}
f_{\left(U,V\right)}\left(u,v\right)
&= f_{\left(X_{\left(1\right)},X_{\left(n+1\right)}\right)}\left(v,u+v\right)
\left|\frac{\partial(x,y)}{\partial(u,v)}\right|\boldsymbol{1}\left(u>0,v>0\right)\\[1mm]
&= n\left(n+1\right)e^{-\left(v+u+v\right)}\left(e^{-v}-e^{-u-v}\right)^{n-1}
\boldsymbol{1}\left(u>0\right)\boldsymbol{1}\left(v>0\right)\\[1mm]
&= n\left(n+1\right)e^{-\left(n+1\right)v-u}\left(1-e^{-u}\right)^{n-1}
\boldsymbol{1}\left(u>0\right)\boldsymbol{1}\left(v>0\right)\\[1mm]&=\left\{ne^{-u}\left(1-e^{-u}\right)^{n-1}\boldsymbol{1}\left(u>0\right)\right\}\left\{\left(n+1\right)e^{-\left(n+1\right)v}\boldsymbol{1}\left(v>0\right)\right\}.
\end{aligned}
$$The joint density separates in $u$ and $v$. Hence, $U,V$ are independent. $U=X_{\left(n+1\right)}-X_{\left(1\right)}$ has density,$$
f_U\left(u\right)=n e^{-u}\left(1-e^{-u}\right)^{n-1}\boldsymbol{1}\left(u>0\right).
$$In an alternative way, $f_U\left(u\right)$ can be written through binomial expansion of $\left(1-e^{-u}\right)^{n-1}$ as, \begin{equation}\label{eq:density}
\begin{aligned}
f_U\left(u\right)
&= ne^{-u}\sum_{k=0}^{n-1}\left(\begin{matrix} n-1 \\ k \end{matrix}\right)
\left(e^{-u}\right)^k\boldsymbol{1}\left(u>0\right)\\[1mm]
&= n\sum_{k=1}^n\left(-1\right)^{k-1}\left(\begin{matrix} n-1 \\ k-1 \end{matrix}\right)
e^{-ku}\boldsymbol{1}\left(u>0\right).
\end{aligned}
\end{equation}In this study, we aim to estimate the density function given in \eqref{eq:density} using the previously mentioned methods in \autoref{sec:intro}. It is important to note that this density corresponds to a random variable representing the range of a sample consisting of $n+1$ observations. To facilitate the application of Edgeworth expansions and Saddle Point approximations, it is necessary to re-express the density in terms of a random variable that can be represented as a sum of particular components. The following theorem, as presented in @feller1971introduction, provides the essential framework for achieving this transformation. Before that we need the following lemma.

\begin{lemma}\label{lem:psisum}
Let $\lambda_0,\lambda_1,\dots,\lambda_s,s\in\mathbb{N}$ be distinct numbers and define
\[
\psi_{k,s}=\prod_{\substack{0\le r\le s \\ r\neq k}}\left(\frac{1}{\lambda_r-\lambda_k}\right).
\]
Then,
\[
\sum_{k=0}^{s}\psi_{k,s}=0.
\]
\end{lemma}
\begin{proof}
Consider the Lagrange basis polynomials
\[
L_k(x)=\prod_{\substack{0\le r\le s \\ r\neq k}}\left(\frac{x-\lambda_r}{\lambda_k-\lambda_r}\right),
\]
which satisfy $L_k(\lambda_j)=\delta_{kj}$ for $0\le j,k\le s$. Since $\left\{L_k(x)\right\}_{k=0}^s$ forms a basis for the space of polynomials of degree at most $s$, we can write the constant function $1$ as
\[
1=\sum_{k=0}^s L_k(x).
\]
Notice that each $L_k(x)$ is a polynomial of degree $s$ whose leading coefficient (the coefficient of $x^s$) is given by
\[
\frac{1}{\prod_{\substack{0\le r\le s \\ r\neq k}}(\lambda_k-\lambda_r)}.
\]
Thus, the coefficient of $x^s$ in the sum $\sum_{k=0}^s L_k(x)$ is
\[
\sum_{k=0}^s \left\{\frac{1}{\prod_{\substack{0\le r\le s \\ r\neq k}}(\lambda_k-\lambda_r)}\right\}.
\]
Since the constant function $1$ has no $x^s$ term, it follows that
\[
\sum_{k=0}^s\left\{ \frac{1}{\prod_{\substack{0\le r\le s \\ r\neq k}}(\lambda_k-\lambda_r)}\right\}=0.
\]
Noting that
\[
\psi_{k,s}=\prod_{\substack{0\le r\le s \\ r\neq k}}\left(\frac{1}{\lambda_r-\lambda_k}\right)
=\; (-1)^s \frac{1}{\prod_{\substack{0\le r\le s \\ r\neq k}}(\lambda_k-\lambda_r)},
\]
we multiply the previous equation by $(-1)^s$ to obtain
\[
\sum_{k=0}^s \psi_{k,s}=(-1)^s \sum_{k=0}^s \left\{\frac{1}{\prod_{\substack{0\le r\le s \\ r\neq k}}(\lambda_k-\lambda_r)}\right\}=0.
\]
This completes the proof.
\end{proof}

Now, we prove the following theorem.

\begin{theorem}[Convolution of Exponential distributions]\label{thm:convexp}
For $j=0,1,\ldots,n$, let $Z_j$ have density $\lambda_je^{-\lambda_jz}$ for $z>0$ where $\lambda_j\neq\lambda_k$ unless $j\neq k$. Put,$$\psi_{k,n}=\prod_{r\neq k}\left(\lambda_r-\lambda_k\right)^{-1},$$ then, $\sum_{j=0}^nZ_j$ has a density given by,$$P_n(t)=\left(\prod_{k=0}^{n}\lambda_{k}\right)\left[\sum_{k=0}^n\psi_{k,n}e^{-\lambda_kt}\right]\,\,\mathrm{for}\,t>0.$$ 
\end{theorem}
\begin{proof}
We will prove the theorem by induction. First, we show the theorem holds for $n=1$.
$Z_j\sim\mathrm{Exponential}\left(\mathrm{rate}=\lambda_j\right)$ independently for $j=0,1.$ Then, cdf of $Z_0+Z_1$ is,
$$
\begin{aligned}
\mathbb{P}\left(Z_0+Z_1\leq t\right) &= \int_{0}^t\int_{0}^{t-x}\lambda_1e^{-\lambda_1y}\lambda_0e^{-\lambda_0x}dydx\\[1mm]&=\int_{0}^t\left[1-e^{-\lambda_1\left(t-x\right)}\right]\lambda_0e^{-\lambda_0x}dx\\[1mm] &=\left[1-e^{-\lambda_0t}\right]-\lambda_0e^{-\lambda_1t}\int_{0}^te^{-\left(\lambda_0-\lambda_1\right)x}dx\\[1mm]&=\left[1-e^{-\lambda_0t}\right]-\frac{\lambda_0}{\left(\lambda_0-\lambda_1\right)}\left[e^{-\lambda_1t}-e^{-\lambda_0t}\right],
\end{aligned}
$$
for $t>0$. Therefore, the pdf of $Z_0+Z_1$ is,
$$
\begin{aligned}
P_1\left(t\right) &= \frac{d}{dt}\left[\mathbb{P}\left(Z_0+Z_1\leq t\right)\right]\\[1mm] &= \lambda_0e^{-\lambda_0t}-\frac{\lambda_0}{\left(\lambda_0-\lambda_1\right)}\left[\lambda_0e^{-\lambda_0t}-\lambda_1e^{-\lambda_1t}\right]\\[1mm] &= \lambda_0\lambda_1\left[\frac{1}{\left(\lambda_1-\lambda_0\right)}e^{-\lambda_0t}+\frac{1}{\left(\lambda_0-\lambda_1\right)}e^{-\lambda_1t}\right]\\&=\left(\prod_{k=0}^1\lambda_k\right)\left[\sum_{k=0}^1\psi_{k,1}e^{-\lambda_kt}\right],
\end{aligned}
$$
for $t>0$. Hence, the theorem holds for $n=1$.
Assume, it holds for $n=s$, i.e., density of $\sum_{j=0}^sZ_j$ is, $$P_s\left(t\right)=\left(\prod_{k=0}^{n}\lambda_{k}\right)\left[\sum_{k=0}^n\psi_{k,n}e^{-\lambda_kt}\right]\,\,\mathrm{for}\,t>0.$$
Then, we have to find the density of $\sum_{j=0}^{s+1}Z_j=\sum_{j=0}^sZ_j+Z_{s+1}$. Note that, $$\psi_{k,s+1}=\psi_{k,s}\left(\lambda_{s+1}-\lambda_k\right)^{-1}.$$ 
Again, following a similar approach, the CDF of $\sum_{j=0}^{s+1}Z_j$ is,
$$
\begin{aligned}
\mathbb{P}\left[\sum_{j=0}^{s+1}Z_j\leq t\right] &= \mathbb{P}\left[\sum_{j=0}^sZ_j+Z_{s+1}\leq t\right]\\[1mm] &= \int_{0}^t\left[\int_{0}^{t-x}\lambda_{s+1}e^{-\lambda_{s+1}y}dy\right]P_s\left(x\right)dx\\[1mm] &= \int_{0}^tP_s\left(x\right)dx-e^{-\lambda_{s+1}t}\int_{0}^{t}e^{\lambda_{s+1}s}P_s\left(x\right)dx.
\end{aligned}
$$
Therefore, pdf of $\sum_{j=0}^{s+1}Z_j$ is,
\begin{align*}
P_{s+1}(t) &= P_s(t) - e^{-\lambda_{s+1}t} e^{\lambda_{s+1}t} P_s(t)
+ e^{-\lambda_{s+1}t}\lambda_{s+1}\int_{0}^{t}e^{\lambda_{s+1}x}P_s(x)\,dx\\[1mm]
&= e^{-\lambda_{s+1}t}\lambda_{s+1}\int_{0}^{t}e^{\lambda_{s+1}x}P_s(x)\,dx\\[1mm]
&= \lambda_{s+1}e^{-\lambda_{s+1}t}\int_{0}^{t}e^{\lambda_{s+1}x}\left(\prod_{k=0}^{s}\lambda_{k}\right)
\left[\sum_{k=0}^{s}\psi_{k,s}e^{-\lambda_kx}\right]dx\\[1mm]
&= -\left(\prod_{k=0}^{s+1}\lambda_{k}\right)e^{-\lambda_{s+1}t}
\sum_{k=0}^{s}\left\{\psi_{k,s}\left(\lambda_{s+1}-\lambda_k\right)^{-1}\right\}
\left\{\int_{0}^{t}\left(\lambda_k-\lambda_{s+1}\right)e^{-(\lambda_k-\lambda_{s+1})x}dx\right\}\\[1mm]
&= -\left(\prod_{k=0}^{s+1}\lambda_{k}\right)e^{-\lambda_{s+1}t}
\sum_{k=0}^{s}\psi_{k,s+1}\left[1-e^{-(\lambda_k-\lambda_{s+1})t}\right]\\[1mm]
&= -\left(\prod_{k=0}^{s+1}\lambda_{k}\right)
\sum_{k=0}^{s}\psi_{k,s+1}\left[e^{-\lambda_{s+1}t}-e^{-\lambda_k t}\right]\\[1mm]
&= \left(\prod_{k=0}^{s+1}\lambda_{k}\right)
\left[\sum_{k=0}^{s}\psi_{k,s+1}e^{-\lambda_k t}-
e^{-\lambda_{s+1}t}\sum_{k=0}^{s}\psi_{k,s+1}\right]\\[1mm]
&= \left(\prod_{k=0}^{s+1}\lambda_{k}\right)
\left[\sum_{k=0}^{s+1}\psi_{k,s+1}e^{-\lambda_k t}-
\psi_{s+1,s+1}e^{-\lambda_{s+1}t}-
e^{-\lambda_{s+1}t}\sum_{k=0}^{s}\psi_{k,s+1}\right]\\[1mm]
&= \left(\prod_{k=0}^{s+1}\lambda_{k}\right)
\left[\sum_{k=0}^{s+1}\psi_{k,s+1}e^{-\lambda_k t}-
e^{-\lambda_{s+1}t}\left\{\sum_{k=0}^{s+1}\psi_{k,s+1}\right\}\right].
\end{align*}
Hence, using \lemref{lem:psisum} we get,
$$P_{s+1}(t)=\left(\prod_{k=0}^{s+1}\lambda_{k}\right)
\left[\sum_{k=0}^{s+1}\psi_{k,s+1}e^{-\lambda_k t}\right].$$
\end{proof}

As a consequence, we have the following corollary.

\begin{corollary}\label{cor:example}
If $Y_j\sim\mathrm{Exponential}\left(\mathrm{mean}=\frac{1}{j}\right)$ independently for all $j=1,2,\ldots,n$ then $S_n=\sum_{j=1}^nY_j$ has \eqref{eq:density} as it's density.
\end{corollary}
\begin{proof}
Say, $Z_j=Y_{j+1}\sim\mathrm{Exponential}\left(\mathrm{rate}=j+1\right)\,\forall j=0,\ldots,n-1$ independently. Then, $\sum_{j=1}^nY_j=\sum_{j=0}^{n-1}Z_j$. Applying \autoref{thm:convexp} on $Z_j$'s by taking $\lambda_j=j+1$ gives, $$P_{n-1}\left(t\right)=n!\sum_{k=0}^{n-1}\psi_{k,n-1}e^{-\left(k+1\right)t},$$ where, $$\psi_{k,n-1}=\frac{\left(-1\right)^{k}}{k!\left(n-1-k\right)!}.$$ Hence, $$P_{n-1}\left(t\right)=n!\sum_{k=0}^{n-1}\frac{\left(-1\right)^{k}}{k!\left(n-1-k\right)!}e^{-\left(k+1\right)t}\\=n\sum_{k=1}^n\left(-1\right)^{k-1}\left(\begin{matrix}n-1\\k-1\end{matrix}\right)e^{-kt},$$ for $t>0$.
\end{proof}

\autoref{cor:example} is also useful for generating samples from \eqref{eq:density}. A brief calculation shows that the cumulant generating function of the sum is $Y_1+Y_2+\ldots+Y_n$ is,$$
-\sum_{j=1}^n\ln\left(1-\frac{t}{j}\right).
$$We have taken $\boldsymbol{n=10}$ for our purpose.

# Kernel Density Estimation {#sec:kde}

Kernel density estimation (KDE) is a popular nonparametric technique for estimating the probability density function of a random variable based on observed data. It works by averaging over a kernel function, which smooths the discrete sample points to form a continuous density curve. In this study, KDE will be used as a baseline method for density estimation, providing a benchmark against which the performance of more advanced approaches can be compared.

Based on a sample $U_1,U_2,\ldots,U_m\overset{iid}{\sim}f_U\left(\centerdot\right)$, the probability density function can be estimated by, \begin{equation}\label{eq:kern}
\hat{f}^{K}_{h_{Ker}}\left(u\right)\coloneqq\frac{1}{m}\sum_{i=1}^m\frac{1}{h_{Ker}}Ker\left(\frac{u-U_i}{h_{Ker}}\right).
\end{equation} Here, $Ker$ is taken to be **Gaussian Kernel**. In order to select the bandwidth $h_{Ker}$ is selected via Least Square Cross Validation (LSCV) which gives $h^{opt}_{Ker}$,$$
h^{opt}_{Ker}\coloneqq\underset{h}{\arg\min}\left[\int_{\mathbb{R}}\left\{\hat{f}^{K}_{h}\left(u\right)\right\}^2du-\frac{2}{m}\sum_{i=1}^m\hat{f}^{K}_{h,-i}\left(U_i\right)\right].
$$We have taken a sample of size $500$ by generating samples using \autoref{thm:convexp} for the estimation procedure with bandwidth selected through LSCV. The estimated density is presented as follows,

```{r,echo=F}
library(ggplot2)
library(Deriv)
library(orthopolynom)
library(magrittr)
set.seed(123)
rangexp=function(u,n=10){
  n*exp(-u)*((1-exp(-u))^(n-1))
}

f <- function(x) {
  rangexp(x, n = n_val)
}
#---Sample generator---
rangesamp=function(m,n=10){
  mat=matrix(rexp(m*(n+1),rate=1),nr=m)
  return(apply(mat,MARGIN = 1,FUN = function(x){return(max(x)-min(x))}))
}
```

```{r,echo=F,fig.cap="PDF of $U$ and it's Kernel Density Estimate"}
#--Kernel Density Estimation---
# Compute the kernel density estimate

x <- rangesamp(m = 500)
bw_ls <- bw.ucv(x)
dens <- density(x, bw = bw_ls, kernel = "gaussian")

# Create a data frame for the kernel density estimate
df_kernel <- data.frame(x = dens$x, density = dens$y)

# Create a grid for the exact density and compute it using rangexp
grid_x <- seq(0, 10, length.out = 1000)
df_exact <- data.frame(x = grid_x, density = rangexp(grid_x))

# Build the ggplot with filled areas, matching borders, and legend on top
p <- ggplot() +
  geom_area(data = df_kernel, 
            aes(x = x, y = density, fill = "Kernel Density", color = "Kernel Density"), 
            alpha = 0.4, linewidth = 1) +
  geom_area(data = df_exact, 
            aes(x = x, y = density, fill = "Exact Density", color = "Exact Density"), 
            alpha = 0.4, linewidth = 1) +
  scale_fill_manual(name = "Density Type",
                    values = c("Kernel Density" = "#0072B2", 
                               "Exact Density" = "#D55E00")) +
  scale_color_manual(name = "Density Type",
                     values = c("Kernel Density" = "#0072B2", 
                                "Exact Density" = "#D55E00")) +
  labs(title = "Kernel Density Estimate",
       x = "x", y = "Density") +
  coord_cartesian(ylim = c(0, 0.42)) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top")

print(p)

```

Although kernel density estimation provided a reasonable approximation, further refinement of the estimate is necessary. Consequently, alternative methods were explored.

# Edgeworth Expansions {#sec:edge}

An Edgeworth Expansion is an asymptotic approximation to a density or distribution function (@hall1992bootstrap). A detailed description on density estimation through Edgeworth expansions can be found in @lange2010numerical for sum of i.i.d. random variables using the cumulant generating function $K$ of the population. But here $Y_j$'s are independent, **not identical**. However, we shall forcefully use this convention,$$
nK\left(t\right)\approx-\sum_{j=1}^n\ln\left(1-\frac{t}{j}\right).
$$From which we get,$$
nK^{\left(k\right)}\left(t\right)=\left(k-1\right)!\sum_{j=1}^n\frac{1}{\left(j-t\right)^k}.
$$Subsequently, $k$th cumulant is, $$
\kappa_{k}=K^{\left(k\right)}\left(0\right).
$$The identities $\kappa_1=\mu$ and $\kappa_2=\sigma^2$ hold in general. For notational convenience, we let $\rho_k=\frac{\kappa_j}{\sigma^j}$. The estimated density of the standardized sum $T_n=\left(\frac{S_n-n\mu}{\sigma}\right)$ is, \begin{equation}\label{eq:sden}
\phi\left(u\right)\left[1+\frac{\rho_3H_3\left(u\right)}{6\sqrt{n}}+\frac{\rho_4H_4\left(u\right)}{24{n}}+\frac{\rho_3^2H_6\left(u\right)}{72{n}}+\mathcal{O}\left(n^{-\frac{3}{2}}\right)\right],
\end{equation} where, $H_k\left(\centerdot\right)$ is the Hermite polynomial of $k$th order expressed as,$$
H_k\left(u\right)=\left(-1\right)^ke^{\frac{1}{2}u^2}\frac{d^k}{du^k}e^{-\frac{1}{2}u^2}\implies\phi\left(u\right)H_k\left(u\right)=\left(-1\right)^k\frac{d^k}{du^k}\phi\left(u\right).
$$Note that, integration of the above identity of the Hermite polynomial gives,$$\begin{aligned}\int_{-\infty}^u\phi\left(x\right)H_k\left(x\right)dx&=\left(-1\right)^k\int_{-\infty}^ud\left(\frac{d^{k-1}}{dx^{k-1}}\phi\left(x\right)\right)\\[1mm]&=\left(-1\right)^{k}\frac{d^{k-1}}{du^{k-1}}\phi\left(u\right)\\[1mm]&=-\phi\left(u\right)H_{k-1}\left(u\right).\end{aligned}$$Which can be used to get $H_n\left(u\right)$'s recursively.

Consequently from \eqref{eq:sden}, density of $S_n$ can be approximated through,\begin{equation}\label{eq:edge}
\hat{f}^{E}\left(u\right)\coloneqq\frac{1}{\sigma}\phi\left(\frac{u-\mu}{\sigma}\right)\left[1+\frac{\rho_3H_3\left(\frac{u-\mu}{\sigma}\right)}{6\sqrt{n}}+\frac{\rho_4H_4\left(\frac{u-\mu}{\sigma}\right)}{24{n}}+\frac{\rho_3^2H_6\left(\frac{u-\mu}{\sigma}\right)}{72{n}}+\mathcal{O}\left(n^{-\frac{3}{2}}\right)\right].
\end{equation}Therefore, if cumulants up to the third order are available, \eqref{eq:density} can be accurately approximated by \eqref{eq:edge}. Moreover, in the presence of a sample, the corresponding sample cumulants may be substituted into \eqref{eq:edge}.

```{r,echo=F}
#--Edgeworth Expansions--
# Define the Hermite function
hermite_eval <- function(order, x, type = c("probabilists", "physicists"), normalized = FALSE) {
  type <- match.arg(type)
  if (order < 0 || order != floor(order)) {
    stop("Order must be a nonnegative integer")
  }
  
  # Generate the polynomials up to the specified order.
  # The list will contain polynomials from order 0 to 'order'.
  if (type == "probabilists") {
    polys <- hermite.he.polynomials(n = order, normalized = normalized)
  } else {
    polys <- hermite.h.polynomials(n = order, normalized = normalized)
  }
  
  # Evaluate each polynomial at the point(s) x.
  # The polynomial of the desired order is at index order + 1.
  eval_list <- polynomial.values(polynomials = polys, x = x)
  result <- eval_list[[order + 1]]
  return(result)
}

#-A function for calculating derivatives--
cgf=function(t,n=10){
  return(sum(sapply(1:n,FUN = function(j){-log(1-(t/j))})))
}

cgfk=function(t,k,n=10){
  return(factorial(k-1)*mean(sapply(1:n,FUN = function(j){1/((j-t)^k)})))
}


den.edge <- function(y, n = 10) {
  # Compute location and scale adjustments
  mun <- sum(sapply(1:n, function(j) { 1/j }))
  sigman <- sqrt(sum(sapply(1:n, function(j) { 1/(j^2) })))
  
  # Standardize input y
  z <- (y - mun) / sigman
  
  # Cumulant generating function helper
  #K
  cgfk <- function(t, k) {
    factorial(k - 1) * mean(sapply(1:n, function(j) { (j - t)^(-k) }))
  }
  
  # Standardized cumulant estimator rho
  rho <- function(r) {
    cgfk(t = 0, k = r) / (cgfk(t = 0, k = 2)^(r / 2))
  }
  
  # Evaluate probabilists' Hermite polynomials at z
  H3 <- hermite_eval(order = 3, x = z, type = "probabilists")
  H4 <- hermite_eval(order = 4, x = z, type = "probabilists")
  H6 <- hermite_eval(order = 6, x = z, type = "probabilists")
  
  # Compute the density with Edgeworth expansion correction
  result <- dnorm(z) * (1 + (rho(3) * H3 / (6 * sqrt(n))) +
                          (rho(4) * H4 / (24 * n)) +
                          ((rho(3)^2) * H6 / (72 * n))) / sigman
  
  # Replace negative values with 0
  result <- pmax(result, 0)
  
  return(result)
}
```

```{r,echo=F,fig.cap="PDF of $U$ and it's approximated density through Edgeworth Expansions"}
# Set parameter value and grid
n_val <- 10
gridp <- seq(0, 10, length.out = 10000)

# Compute densities:
df_edge <- data.frame(
  x = gridp,
  density = den.edge(gridp, n = n_val),
  Method = "Edgeworth Expansion"
)

df_exact <- data.frame(
  x = gridp,
  density = rangexp(gridp, n = n_val),
  Method = "Exact Density"
)

# Build the ggplot with filled areas and matching borders
p <- ggplot() +
  geom_area(data = df_edge, 
            aes(x = x, y = density, fill = "Edgeworth Expansion", color = "Edgeworth Expansion"),
            alpha = 0.4, linewidth = 1) +
  geom_area(data = df_exact, 
            aes(x = x, y = density, fill = "Exact Density", color = "Exact Density"),
            alpha = 0.4, linewidth = 1) +
  scale_fill_manual(name = "Density Type",
                    values = c("Edgeworth Expansion" = "purple",
                               "Exact Density" = "#D55E00")) +
  scale_color_manual(name = "Density Type",
                     values = c("Edgeworth Expansion" = "purple",
                                "Exact Density" = "#D55E00")) +
  labs(title = "Approximation through Edgeworth Expansions",
       x = "x", y = "Density") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top")

print(p)
```

\FloatBarrier

However, this procedure didn't worked considerably well near the tails. That's why we go for saddle point approximations.

# Saddle Point approximations {#sec:saddle}

The expansion \eqref{eq:edge} suggest that the rate of convergence of $S_n$ to normality is governed by the correction term $\mathcal{O}\left(n^{-\frac{1}{2}}\right)$. However, this pessimistic impression is misleading at $u=0$ because $H_3\left(0\right)=0$ (@lange2010numerical). The device known as exponential titling exploits this peculiarity (@barndorff1989asymptotic).

The procedure of estimating $f_U\left(\centerdot\right)$ at $u$ through exponential titling has been described in @lange2010numerical. Which gives Saddle Point approximation I using \eqref{eq:edge} as, \begin{equation}\label{eq:saddleI}
\hat{f}^{I}\left(u\right)\coloneqq\frac{e^{-t_0u+nK\left(t_0\right)}}{\sqrt{2\pi nK''\left(t_0\right)}}\left[1+\mathcal{O}\left(n^{-1}\right)\right],
\end{equation} where, $t_0$ satisfies, $$
\sum_{j=1}^n\frac{1}{\left(j-t_0\right)}=u.
$$This equation need to be solved numerically. The saddle point approximated density is presented as follows,

```{r,echo=F}
den.saddle1 <- function(x0, n = 10) {
  # Cumulant generating function
  cgf <- function(t) {
    sum(-log(1 - t / (1:n)))
  }
  
  # Second derivative of CGF
  cgfk2 <- function(t) {
    sum((1:n - t)^(-2))
  }
  
  # Compute density for each x0
  result <- sapply(x0, function(x) {
    # Saddlepoint equation: K'(t) - x = 0
    f <- function(t) sum(1 / (1:n - t)) - x
    
    # Find root in (-Inf, 1)
    t_0 <- tryCatch(
      uniroot(f, c(-1e6, 1 - 1e-6))$root,
      error = function(e) NA
    )
    
    if (is.na(t_0)) return(NA)
    
    # Saddlepoint approximation formula
    exp(cgf(t_0) - t_0 * x) / sqrt(2 * pi * cgfk2(t_0))
  })
  
  return(result)
}
```

```{r,echo=F,fig.cap="PDF of $U$ and it's approximated density through Saddlepoint Approximation 1"}
options(warn=-1)
#---Saddle Point approximations 1----
# Compute densities:
df_saddle <- data.frame(
  x = gridp,
  density = den.saddle1(gridp, n = n_val),
  Method = "Saddlepoint Approximation 1"
)


# Build the ggplot with filled areas and matching borders
p <- ggplot() +
  geom_area(data = df_saddle, 
            aes(x = x, y = density, fill = "Saddlepoint Approximation 1", color = "Saddlepoint Approximation 1"), 
            alpha = 0.4, linewidth = 1) +
  geom_area(data = df_exact, 
            aes(x = x, y = density, fill = "Exact Density", color = "Exact Density"), 
            alpha = 0.4, linewidth = 1) +
  scale_fill_manual(name = "Density Type",
                    values = c("Saddlepoint Approximation 1" = "lightgreen", 
                               "Exact Density" = "#D55E00")) +
  scale_color_manual(name = "Density Type",
                     values = c("Saddlepoint Approximation 1" = "lightgreen", 
                                "Exact Density" = "#D55E00")) +
  labs(title = "Approximation through Saddlepoint Approximation 1",
       x = "x", y = "Density") +
  coord_cartesian(ylim = c(0, max(c(df_saddle$density, df_exact$density)))) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top")

print(p)
```

\FloatBarrier

Further terms can be included in \eqref{eq:saddleI} if we substitute the appropriate normalized cumulants,$$
\rho_k\left(t_0\right)=\frac{nK^{\left(k\right)}\left(t_0\right)}{\left[nK''\left(t_0\right)\right]^{\frac{k}{2}}}.
$$ of the tilted density in the Edgeworth expansion \eqref{eq:edge}. Once we determine $H_3\left(0\right)=0,H_4\left(0\right)=3$ and $H_6\left(0\right)=-15$, it is obvious that, \begin{equation}\label{eq:saddleII}
\hat{f}^{II}\left(u\right)\coloneqq\frac{e^{-t_0u+nK\left(t_0\right)}}{\sqrt{2\pi nK''\left(t_0\right)}}\left[1+\frac{3\rho_4\left(t_0\right)-5\rho_3^2\left(t_0\right)}{24n}+\mathcal{O}\left(n^{-\frac{3}{2}}\right)\right].
\end{equation} Equation \eqref{eq:saddleII} is known as Saddle Point approximation II to \eqref{eq:density}.

```{r,echo=F}
den.saddle2 <- function(x0, n = 10) {
  # Cumulant generating function
  #nK
  cgf <- function(t) {
    sum(-log(1 - t / (1:n)))
  }
  
  #K
  cgfk <- function(t, k) {
    factorial(k - 1) * mean(sapply(1:n, function(j) { (j - t)^(-k) }))
  }
  
  
  rho <- function(t,r) {
    n*cgfk(t = t, k = r) / ((cgfk(t = t, k = 2)*n)^(r / 2))
  }
  
  # Compute density for each x0
  result <- sapply(x0, function(x) {
    # Saddlepoint equation: K'(t) - x = 0
    f <- function(t) sum(1 / (1:n - t)) - x
    
    # Find root in (-Inf, 1)
    t_0 <- tryCatch(
      uniroot(f, c(-1e6, 1 - 1e-6))$root,
      error = function(e) NA
    )
    
    if (is.na(t_0)) return(NA)
    
    # Saddlepoint approximation formula
    (exp(cgf(t_0) - t_0 * x) / sqrt(2 * pi *n* cgfk(t=t_0,k=2)))*(1+((3*rho(t=t_0,r=4)-5*rho(t=t_0,r=3)^2)/(24*n)))
  })
  
  return(result)
}
```

```{r,echo=F,fig.cap="PDF of $U$ and it's estimated density through Saddlepoint Approximation 2",fig.height=2.8}
options(warn=-1)
# Compute densities
df_saddle2 <- data.frame(
  x = gridp,
  density = den.saddle2(gridp, n = n_val),
  Method = "Saddlepoint Approximation 2"
)

df_exact <- data.frame(
  x = gridp,
  density = rangexp(gridp, n = n_val),
  Method = "Exact Density"
)

# Build the ggplot with filled areas and matching borders
p <- ggplot() +
  geom_area(data = df_saddle2, 
            aes(x = x, y = density, fill = "Saddlepoint Approximation 2", color = "Saddlepoint Approximation 2"),
            alpha = 0.4, linewidth = 1) +
  geom_area(data = df_exact, 
            aes(x = x, y = density, fill = "Exact Density", color = "Exact Density"),
            alpha = 0.4, linewidth = 1) +
  scale_fill_manual(name = "Density Type",
                    values = c("Saddlepoint Approximation 2" = "tomato", 
                               "Exact Density" = "#D55E00")) +
  scale_color_manual(name = "Density Type",
                     values = c("Saddlepoint Approximation 2" = "tomato", 
                                "Exact Density" = "#D55E00")) +
  labs(title = "Approximation through Saddlepoint Approximation 2",
       x = "x", y = "Density") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top",
        legend.text = element_text(size = 9),
        legend.title = element_text(size = 10),
        legend.key.size = unit(0.6, "lines"),
        plot.title = element_text(size = 11))

print(p)
```

\FloatBarrier

However, saddle point approximations outperformed Edgeworth approximations. Note that, throughout we assumed that the cumulant generating function is completely known. But, this assumption in unrealistic, we'll only have a sample.

# Aggregation

For better comparison, we have superposed all the plots.

```{r,echo=F}
density_at_new <- function(x_new, data, h, kernel=dnorm) {
  sapply(x_new, function(x0) {
    mean(kernel((x0 - data) / h)) / h
  })
}
```

```{r,echo=F,fig.cap="PDF of $U$ and it's density estimates",fig.height=3.4}
p <- ggplot() +
  geom_area(data = df_kernel, 
            aes(x = x, y = density, fill = "Kernel Density", color = "Kernel Density"), 
            alpha = 0.4, linewidth = 1) +
  geom_area(data = df_exact, 
            aes(x = x, y = density, fill = "Exact Density", color = "Exact Density"), 
            alpha = 0.4, linewidth = 1) +
  geom_area(data = df_edge, 
            aes(x = x, y = density, fill = "Edgeworth Expansion", color = "Edgeworth Expansion"),
            alpha = 0.4, linewidth = 1)+
  geom_area(data = df_saddle, 
            aes(x = x, y = density, fill = "Saddlepoint Approximation 1", color = "Saddlepoint Approximation 1"), 
            alpha = 0.4, linewidth = 1)+
  geom_area(data = df_saddle2, 
            aes(x = x, y = density, fill = "Saddlepoint Approximation 2", color = "Saddlepoint Approximation 2"),
            alpha = 0.4, linewidth = 1)+
  scale_fill_manual(name = "Density Type",
                    values = c("Kernel Density" = "#0072B2", 
                               "Exact Density" = "#D55E00",
                               "Edgeworth Expansion" = "purple",
                               "Saddlepoint Approximation 1"="lightgreen",
                               "Saddlepoint Approximation 2"="tomato")) +
  scale_color_manual(name = "Density Type",
                     values = c("Kernel Density" = "#0072B2", 
                                "Exact Density" = "#D55E00",
                                "Edgeworth Expansion" = "purple",
                                "Saddlepoint Approximation 1"="lightgreen",
                                "Saddlepoint Approximation 2"="tomato")) +
  labs(title = "Density Estimates",
       x = "x", y = "Density") +
  coord_cartesian(ylim = c(0, 0.42)) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top",
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 9.5),
        legend.key.size = unit(0.5, "lines"),
        plot.title = element_text(size = 11)) +
  guides(fill = guide_legend(ncol = 2),
         color = guide_legend(ncol = 2
                              ))

print(p)
```

\FloatBarrier

The following table displays the exact density \eqref{eq:density} and the errors (absolute difference between exact values and approximate values) committed in using Gaussian Kernel Density Estimation \eqref{eq:kern}, Edgeworth Expansion \eqref{eq:edge}, and the two saddle point expansions \eqref{eq:saddleI} and \eqref{eq:saddleII} when $n=10$.

```{r,echo=F}
x0=seq(from=0.5,to=9,by=0.5)
den.exact=rangexp(x0)
den.kernel=density_at_new(x_new = x0,data = x, h= bw.ucv(x))
den.edgeworth=den.edge(y=x0)
df=data.frame("x0"=x0,"Exact"=den.exact,
              "Error.Kern"=abs(den.exact-den.kernel),
              "Error.Edgeworth"=abs(den.exact-den.edgeworth),
              "Error.Saddle1"=abs(den.exact-den.saddle1(x0)),
              "Error.Saddle2"=abs(den.exact-den.saddle2(x0)))
df=round(df,5)
```

```{r,echo=F}
# Load required packages
library(knitr)
library(kableExtra)

# Set column names
colnames(df) <- c("\\textbf{$x_0$}", 
                  "\\textbf{$f_U(x_0)$}", 
                  "\\textbf{Error \\eqref{eq:kern}}", 
                  "\\textbf{Error \\eqref{eq:edge}}", 
                  "\\textbf{Error \\eqref{eq:saddleI}}", 
                  "\\textbf{Error \\eqref{eq:saddleII}}")

# Create the LaTeX table
kable(df,
      format = "latex", 
      escape = FALSE, 
      booktabs = TRUE,
      caption = "Comparison of Density Approximation Errors",
      align = "c") %>%
  kable_styling(latex_options = c("striped", "hold_position"))

```

Both saddle point expansions clearly outperform the Edgeworth expansion except very close to the mean $\sum_{j=1}^{10}\frac{1}{j}\approx2.93$. Indeed, it's remarkable how well the saddle point expansions do considering how far this example is from the ideal of a sum of i.i.d. random variables. The refined saddle point expansion \eqref{eq:saddleII} is an improvement over the ordinary saddle point expansion \eqref{eq:saddleI} in the tails of the density but not necessarily in the center. @daniels1987saddle considers variation on this problem involving pure birth processes.

Remember, KDE is based on a sample, which is more realistic scenario then the other two methods, which assumes the CGF to be known. So, they are not comparable with KDE at any stage.

# Conclusion

This study has systematically compared kernel density estimation (KDE), Edgeworth expansions, and saddle point approximations for estimating and approximating the density of a sample range from an exponential distribution, revealing distinct trade-offs: KDE excels in data-driven flexibility, making it practically viable, whereas Edgeworth expansions and saddle point approximations, despite offering theoretical refinements—such as improved tail accuracy in the latter—lack practical utility due to their reliance on a known cumulant generating function, an unrealistic assumption in real-world scenarios. These findings, aggregated through density plots and error comparisons, provide a framework for method selection, emphasizing KDE’s applicability. Future work could explore hybrid approaches to bridge theoretical precision with practical use.

Entire analysis has been implemented in R. You can find the codes on \href{https://github.com/SB2003ISI/DensityEstimation/tree/main}{\textcolor{NavyBlue}{\faGithub\,GitHub}}.

------------------------------------------------------------------------
